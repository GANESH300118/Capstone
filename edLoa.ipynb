{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `learning_rate` should be float, or an instance of LearningRateSchedule, or a callable (that takes in the current iteration value and returns the corresponding learning rate value). Received instead: learning_rate=Lion",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 131\u001b[0m\n\u001b[1;32m    128\u001b[0m model \u001b[38;5;241m=\u001b[39m build_edensenet((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m3\u001b[39m), num_classes)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# Initialize Lion optimizer\u001b[39;00m\n\u001b[0;32m--> 131\u001b[0m lion_optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mLion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Compile the model with the Lion optimizer\u001b[39;00m\n\u001b[1;32m    134\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mlion_optimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[2], line 74\u001b[0m, in \u001b[0;36mLion.__init__\u001b[0;34m(self, learning_rate, beta_1, beta_2, name, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, beta_1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, beta_2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.99\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mLion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# Handle the learning rate initialization\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_learning_rate(learning_rate)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gg2/lib/python3.10/site-packages/keras/src/backend/tensorflow/optimizer.py:22\u001b[0m, in \u001b[0;36mTFOptimizer.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribution_strategy \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mget_strategy()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/gg2/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:124\u001b[0m, in \u001b[0;36mBaseOptimizer.__init__\u001b[0;34m(self, learning_rate, weight_decay, clipnorm, clipvalue, global_clipnorm, use_ema, ema_momentum, ema_overwrite_frequency, loss_scale_factor, gradient_accumulation_steps, name, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(learning_rate, \u001b[38;5;28mfloat\u001b[39m):\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    125\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `learning_rate` should be float, or an instance \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof LearningRateSchedule, or a callable \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    127\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(that takes in the current iteration value \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand returns the corresponding learning rate value). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    129\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived instead: learning_rate=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlearning_rate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         )\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mname_scope(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname, caller\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    132\u001b[0m         learning_rate \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mVariable(\n\u001b[1;32m    133\u001b[0m             learning_rate,\n\u001b[1;32m    134\u001b[0m             name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    137\u001b[0m             aggregation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_first_replica\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    138\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `learning_rate` should be float, or an instance of LearningRateSchedule, or a callable (that takes in the current iteration value and returns the corresponding learning rate value). Received instead: learning_rate=Lion"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Optimizer\n",
    "\n",
    "# Set parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "CSV_PATH = '/Users/pillala/Documents/capstone project/messidor-2/messidor_data.csv'\n",
    "SEGMENTED_FOLDER = '/Users/pillala/Documents/capstone project/messidor-2/segmented_images/'\n",
    "\n",
    "# 1. Load CSV data and filter gradable images\n",
    "data = pd.read_csv(CSV_PATH)\n",
    "data = data[data['adjudicated_gradable'] == 1]\n",
    "image_paths = data['id_code'].apply(lambda x: os.path.join(SEGMENTED_FOLDER, x)).values\n",
    "labels = data['diagnosis'].values\n",
    "\n",
    "# 2. One-hot encode the labels\n",
    "num_classes = len(np.unique(labels))\n",
    "labels = to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# 3. Data Generator Class\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size, img_size, shuffle=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_paths = self.image_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        images = [self.load_image(path) for path in batch_paths]\n",
    "        return np.array(images), np.array(batch_labels)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            temp = list(zip(self.image_paths, self.labels))\n",
    "            np.random.shuffle(temp)\n",
    "            self.image_paths, self.labels = zip(*temp)\n",
    "\n",
    "    def load_image(self, path):\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img, self.img_size)\n",
    "        return img / 255.0\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(image_paths, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_generator = DataGenerator(X_train, y_train, BATCH_SIZE, IMG_SIZE)\n",
    "val_generator = DataGenerator(X_val, y_val, BATCH_SIZE, IMG_SIZE)\n",
    "\n",
    "# 4. Define the Lion Optimizer\n",
    "class Lion(Optimizer):\n",
    "    def __init__(self, learning_rate=0.001, beta_1=0.9, beta_2=0.99, name=\"Lion\", **kwargs):\n",
    "        super(Lion, self).__init__(name, **kwargs)\n",
    "\n",
    "        # Handle the learning rate initialization\n",
    "        self._learning_rate = self._build_learning_rate(learning_rate)\n",
    "        self.beta_1 = beta_1\n",
    "        self.beta_2 = beta_2\n",
    "\n",
    "    def _build_learning_rate(self, lr):\n",
    "        if isinstance(lr, (float, int)):\n",
    "            return tf.Variable(lr, name=\"learning_rate\", trainable=False)\n",
    "        elif isinstance(lr, tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "            return lr\n",
    "        else:\n",
    "            raise ValueError(\"learning_rate must be a float, int, or LearningRateSchedule.\")\n",
    "\n",
    "    def _create_slots(self, var_list):\n",
    "        for var in var_list:\n",
    "            self.add_slot(var, \"m\")\n",
    "\n",
    "    def _resource_apply_dense(self, grad, var, apply_state=None):\n",
    "        lr_t = tf.convert_to_tensor(self._learning_rate, var.dtype)\n",
    "        m = self.get_slot(var, \"m\")\n",
    "\n",
    "        new_m = self.beta_1 * m + (1 - self.beta_1) * tf.sign(grad)\n",
    "        var_update = self.beta_2 * var - lr_t * new_m\n",
    "\n",
    "        updates = [m.assign(new_m), var.assign(var_update)]\n",
    "        return tf.group(*updates)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Lion, self).get_config()\n",
    "        config.update({\n",
    "            \"learning_rate\": self._learning_rate.numpy(),\n",
    "            \"beta_1\": self.beta_1,\n",
    "            \"beta_2\": self.beta_2,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "# 5. Define the EDenseNet Model\n",
    "def build_edensenet(input_shape, num_classes):\n",
    "    base_model = DenseNet121(input_shape=input_shape, include_top=False, weights='imagenet', name='edensenet')\n",
    "    base_model.trainable = False  # Freeze the base model\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Initialize the model\n",
    "model = build_edensenet((224, 224, 3), num_classes)\n",
    "\n",
    "# Initialize Lion optimizer\n",
    "lion_optimizer = Lion(learning_rate=LEARNING_RATE)\n",
    "\n",
    "# Compile the model with the Lion optimizer\n",
    "model.compile(optimizer=lion_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 6. Train the model\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1)\n",
    "\n",
    "# 7. Evaluate the model\n",
    "y_pred = np.argmax(model.predict(val_generator), axis=1)\n",
    "y_true = np.argmax(np.concatenate([y for _, y in val_generator], axis=0), axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, zero_division=1))\n",
    "\n",
    "# 8. Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# 9. Plot Training and Validation Accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 10. Plot Training and Validation Loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gg2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
