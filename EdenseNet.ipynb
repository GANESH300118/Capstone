{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "# Set parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "CSV_PATH = '/Users/pillala/Documents/capstone project/messidor-2/messidor_data.csv'\n",
    "SEGMENTED_FOLDER = '/Users/pillala/Documents/capstone project/messidor-2/segmented_images'\n",
    "\n",
    "# 1. Load CSV data and filter gradable images\n",
    "data = pd.read_csv(CSV_PATH)\n",
    "data = data[data['adjudicated_gradable'] == 1]\n",
    "\n",
    "# Image paths and labels\n",
    "image_paths = data['id_code'].apply(lambda x: os.path.join(SEGMENTED_FOLDER, x)).values\n",
    "labels = data['diagnosis'].values\n",
    "num_classes = len(np.unique(labels))\n",
    "labels = to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# 2. Define a Data Generator\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size, img_size, shuffle=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_paths = self.image_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        images = [self.load_image(path) for path in batch_paths]\n",
    "        images = np.array(images)\n",
    "        labels = np.array(batch_labels)\n",
    "        return images, labels\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            temp = list(zip(self.image_paths, self.labels))\n",
    "            np.random.shuffle(temp)\n",
    "            self.image_paths, self.labels = zip(*temp)\n",
    "    \n",
    "    def load_image(self, image_path):\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.resize(img, self.img_size)\n",
    "        return img / 255.0\n",
    "\n",
    "# Split into training and validation sets\n",
    "split_idx = int(0.8 * len(image_paths))\n",
    "train_paths, val_paths = image_paths[:split_idx], image_paths[split_idx:]\n",
    "train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "train_generator = DataGenerator(train_paths, train_labels, BATCH_SIZE, IMG_SIZE)\n",
    "val_generator = DataGenerator(val_paths, val_labels, BATCH_SIZE, IMG_SIZE)\n",
    "\n",
    "# 3. Define the EDenseNet Model\n",
    "def build_edensenet(input_shape, num_classes):\n",
    "    base_model = DenseNet121(input_shape=input_shape, include_top=False, weights='imagenet', name='edensenet')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Initialize and compile the model\n",
    "model = build_edensenet((224, 224, 3), num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, callbacks=callbacks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical, Sequence\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "# Set parameters\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.0001\n",
    "CSV_PATH = 'F:/DRXAI/messidor_data.csv'\n",
    "SEGMENTED_FOLDER = 'F:/DRXAI/Segmented_Images'\n",
    "\n",
    "# 1. Load CSV data and filter gradable images\n",
    "data = pd.read_csv(CSV_PATH)\n",
    "data = data[data['adjudicated_gradable'] == 1]\n",
    "\n",
    "# Image paths and labels\n",
    "image_paths = data['id_code'].apply(lambda x: os.path.join(SEGMENTED_FOLDER, x)).values\n",
    "labels = data['diagnosis'].values\n",
    "num_classes = len(np.unique(labels))\n",
    "labels = to_categorical(labels, num_classes=num_classes)\n",
    "\n",
    "# 2. Define a Data Generator\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_paths, labels, batch_size, img_size, shuffle=True):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_paths = self.image_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        batch_labels = self.labels[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        \n",
    "        images = [self.load_image(path) for path in batch_paths]\n",
    "        images = np.array(images)\n",
    "        labels = np.array(batch_labels)\n",
    "        return images, labels\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            temp = list(zip(self.image_paths, self.labels))\n",
    "            np.random.shuffle(temp)\n",
    "            self.image_paths, self.labels = zip(*temp)\n",
    "    \n",
    "    def load_image(self, image_path):\n",
    "        img = cv2.imread(image_path)\n",
    "        img = cv2.resize(img, self.img_size)\n",
    "        return img / 255.0\n",
    "\n",
    "# Split into training and validation sets\n",
    "split_idx = int(0.8 * len(image_paths))\n",
    "train_paths, val_paths = image_paths[:split_idx], image_paths[split_idx:]\n",
    "train_labels, val_labels = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "train_generator = DataGenerator(train_paths, train_labels, BATCH_SIZE, IMG_SIZE)\n",
    "val_generator = DataGenerator(val_paths, val_labels, BATCH_SIZE, IMG_SIZE)\n",
    "\n",
    "# 3. Define the EDenseNet Model\n",
    "def build_edensenet(input_shape, num_classes):\n",
    "    base_model = DenseNet121(input_shape=input_shape, include_top=False, weights='imagenet', name='edensenet')\n",
    "    base_model.trainable = False\n",
    "\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=output)\n",
    "    return model\n",
    "\n",
    "# Initialize and compile the model\n",
    "model = build_edensenet((224, 224, 3), num_classes)\n",
    "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE),\n",
    "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Callbacks\n",
    "callbacks = [\n",
    "    # EarlyStopping(patience=10, restore_best_weights=True, verbose=1),\n",
    "    # ReduceLROnPlateau(factor=0.1, patience=5, min_lr=1e-6, verbose=1)\n",
    "]\n",
    "\n",
    "# 5. Train the model\n",
    "history = model.fit(train_generator, validation_data=val_generator, epochs=EPOCHS, callbacks=callbacks)\n",
    "\n",
    "# 6. Evaluate the model\n",
    "y_pred = np.argmax(model.predict(val_generator), axis=1)\n",
    "y_true = np.argmax(np.vstack([y for _, y in val_generator]), axis=1)\n",
    "\n",
    "print(classification_report(y_true, y_pred, zero_division=1))\n",
    "\n",
    "# 7. Confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# 8. Plot accuracy\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 9. Plot loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gg2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
